{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp(first~last): 2023-01-01 00:00:00 ~ 2024-10-14 10:00:00\n",
      "timestamp(min~max):    2023-01-01 00:00:00 ~ 2024-10-14 10:00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "#from lightgbm import LGBMRegressor  # sckit에서 lightgbm을 wrapping한 것\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import Utility as Util\n",
    "\n",
    "########\n",
    "buildingID, buildingName = 'B0001', '해양과학고'\n",
    "file_path = '/home/ymatics/CodingSpace/2024_AI_BEMS/241014_data_colec_h/data_colec_h_202410141025_B0001.csv'\n",
    "#########\n",
    "df_buildID = pd.read_csv(file_path)\n",
    "df_buildID['colec_dt'] = pd.to_datetime(df_buildID['colec_dt']).dt.floor('min')  # 분 이하는 제거\n",
    "print(f\"timestamp(first~last): {df_buildID['colec_dt'].iloc[0]} ~ {df_buildID['colec_dt'].iloc[-1]}\")\n",
    "print(f\"timestamp(min~max):    {df_buildID['colec_dt'].min()} ~ {df_buildID['colec_dt'].max()}\")\n",
    "\n",
    "start_date, end_date = pd.to_datetime('2023-01-01 00:00:00'), pd.to_datetime('2024-10-14 10:00:00')\n",
    "df_raw_all = df_buildID[(df_buildID['colec_dt'] >= start_date) & (df_buildID['colec_dt'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### devID, tagCD extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagCD list:  [100001 100002 100003 100004 100005 100006 100007 202202]\n"
     ]
    }
   ],
   "source": [
    "# 관찰 구간\n",
    "#devID==6001~6083, 6085~6149 \"에어컨\", tagCD==?????? '??????', anomaly_detection(), vacation_detection(), optimal_control()\n",
    "#100001\t운전선택탭  off: 0, on: 1  <-- 이산치인데 데이터누락이 많음음\n",
    "#100002\t냉난방모드\tcool: 0, heat: 1, dry: 2, fan: 3, auto: 4, normal: 5, env: 6, sleep: 6\n",
    "#100003\t현재온도\t현재온도\n",
    "#100004\t희망온도\t냉방 18.0~30.0, 난방 16.0~30.0\n",
    "#100005\t풍량        low: 0, med: 1, mid: 1, high: 2, auto: 3\n",
    "#100006\t리모컨허용  level: 2, true: 1, false: 0\n",
    "#100007\t에러\t    indoor: 0, outdoor: 1, caur: 2, trans: 3\n",
    "\n",
    "devID, device_name = 6037, '행정실 에어컨'\n",
    "\n",
    "Util.print_tagCD(df_raw_all, devID)# \n",
    "tag_dict = {100001: '운전선택탭', 100002: '냉난방모드', 100003: '현재온도', 100004: '희망온도'}\n",
    "# dictionary를 이용해 각 태그 데이터를 선택\n",
    "tags = {key: Util.select_devID_tagCD(df_raw_all, devID, tagCD=key) for key in tag_dict.keys()}\n",
    "# 특정 태그에 대해 추가 연산 수행 (예: 전체누적사용량 차이 계산)\n",
    "#tags[90004] = calc_tagCD_diff(tags[90004])\n",
    "\n",
    "# 그래프를 그리기 위해 필요한 데이터를 리스트로 변환\n",
    "tag_data = [tags[key] for key in tag_dict.keys()]\n",
    "tag_names = [tag_dict[key] for key in tag_dict.keys()]\n",
    "\n",
    "Util.plot_dfL_devID_tagCD(tag_data, tag_names, device_name, createFig=False, W=None, H=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step A. Feature Engineering, lightGBM Modeling\n",
    "  - LightGBM은 tree-based 모델로 scaling을 특별히 요구하지 않음\n",
    "  - Prediction model 일 때는, 과거 데이터 정보(X)로만 현재 데이터(y)를 Regression 할 수 있도록 함\n",
    "  - 일반적인 Regression model 일 때는, 타 데이터의 현재 데이터(X)를 사용해도 됨\n",
    "  - 참고로, LSTM은 (t-k, ..., t-1) ---> t 에서 별도의 feature engineering을 수행하지 않음 \n",
    "  - train_test_split() 에서, prediction model에서는 shuffle=False, regression model에서는 shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import B0001_Aircon_DataProcessing_241103 as DP\n",
    "\n",
    "# 데이터 정제\n",
    "df_offON = pd.DataFrame({'colec_dt': tag_data[0]['colec_dt'], 'colec_val': tag_data[0]['colec_val']+15})\n",
    "df_coolHeat = pd.DataFrame({'colec_dt': tag_data[1]['colec_dt'], 'colec_val': tag_data[1]['colec_val']+10})\n",
    "df_tCur = pd.DataFrame({'colec_dt': tag_data[2]['colec_dt'], 'colec_val': tag_data[2]['colec_val']})\n",
    "df_tSet = pd.DataFrame({'colec_dt': tag_data[3]['colec_dt'], 'colec_val': tag_data[3]['colec_val']})\n",
    "\n",
    "# 음수 보정 --> 음수는 결측치로 처리하고, 이후 한꺼번에 보간 처리\n",
    "df_offON.loc[df_offON['colec_val'] < 0, 'colec_val'] = np.nan\n",
    "df_coolHeat.loc[df_coolHeat['colec_val'] < 0, 'colec_val'] = np.nan\n",
    "df_tCur.loc[df_tCur['colec_val'] < 0, 'colec_val'] = np.nan\n",
    "df_tSet.loc[df_tSet['colec_val'] < 0, 'colec_val'] = np.nan\n",
    "\n",
    "#####\n",
    "start_date, end_date = '2023-06-01', '2023-07-01'\n",
    "#####\n",
    "\n",
    "df_offON_sliced = df_offON[(df_offON['colec_dt'] >= start_date) & (df_offON['colec_dt'] <= end_date)]\n",
    "df_coolHeat_sliced = df_coolHeat[(df_coolHeat['colec_dt'] >= start_date) & (df_coolHeat['colec_dt'] <= end_date)]\n",
    "df_tCur_sliced = df_tCur[(df_tCur['colec_dt'] >= start_date) & (df_tCur['colec_dt'] <= end_date)]\n",
    "df_tSet_sliced = df_tSet[(df_tSet['colec_dt'] >= start_date) & (df_tSet['colec_dt'] <= end_date)]   \n",
    "\n",
    "#Util.plot_dfL_devID_tagCD([df_offON_sliced, df_coolHeat_sliced, df_tCur_sliced, df_tSet_sliced], tag_names, device_name, createFig=True, W=None, H=600)\n",
    "\n",
    "############\n",
    "# 데이터 정제\n",
    "df_offON_raw = pd.DataFrame(data=tag_data[0]['colec_val'].values, index=tag_data[0]['colec_dt'], columns=['value'])\n",
    "df_coolHeat_raw = pd.DataFrame(data=tag_data[1]['colec_val'].values, index=tag_data[1]['colec_dt'], columns=['value'])\n",
    "df_tCur_raw = pd.DataFrame(data=tag_data[2]['colec_val'].values, index=tag_data[2]['colec_dt'], columns=['value'])\n",
    "df_tSet_raw = pd.DataFrame(data=tag_data[3]['colec_val'].values, index=tag_data[3]['colec_dt'], columns=['value'])\n",
    "# 음수 보정 --> 음수는 결측치로 처리하고, 이후 한꺼번에 보간 처리\n",
    "df_offON_raw.loc[df_offON_raw['value'] < 0, 'value'] = np.nan\n",
    "df_coolHeat_raw.loc[df_coolHeat_raw['value'] < 0, 'value'] = np.nan\n",
    "df_tCur_raw.loc[df_tCur_raw['value'] < 0, 'value'] = np.nan\n",
    "df_tSet_raw.loc[df_tSet_raw['value'] < 0, 'value'] = np.nan\n",
    "\n",
    "df_offON, df_offON_is_missing, df_offON_nan_counts_df, df_offON_missing_ratio = DP.preprocess(df_offON_raw, points=4, freqInterval='15min', only_cleansing=True, fill_method='time')\n",
    "df_offON.loc[df_offON['value'] > 0, 'value'] = 1\n",
    "# df_offON.value가 1를 유지하다가 0로 떨어지는 지점을 1로 셋팅\n",
    "df_offON['value'] = df_offON['value'].mask((df_offON['value'] == 0) & (df_offON['value'].shift(1) == 1), 1)\n",
    "\n",
    "df_coolHeat, df_coolHeat_is_missing, df_coolHeat_nan_counts_df, df_coolHeat_missing_ratio = DP.preprocess(df_coolHeat_raw, points=4, freqInterval='15min', only_cleansing=True, fill_method='time')\n",
    "df_tCur, df_tCur_is_missing, df_tCur_nan_counts_df, df_tCur_missing_ratio = DP.preprocess(df_tCur_raw, points=4, freqInterval='15min', only_cleansing=True, fill_method='time')\n",
    "df_tSet, df_tSet_is_missing, df_tSet_nan_counts_df, df_tSet_missing_ratio = DP.preprocess(df_tSet_raw, points=4, freqInterval='15min', only_cleansing=True, fill_method='time')\n",
    "\n",
    "df_offON_virtual = DP.generate_df_offON_virtual(df_tCur, df_coolHeat, df_offON)\n",
    "###\n",
    "\n",
    "df_offON_sliced = df_offON[(df_offON.index >= start_date) & (df_offON.index <= end_date)] + 18\n",
    "df_offON_virtual_sliced = df_offON_virtual[(df_offON_virtual.index >= start_date) & (df_offON_virtual.index <= end_date)] + 18\n",
    "\n",
    "df_coolHeat_sliced = df_coolHeat[(df_coolHeat.index >= start_date) & (df_coolHeat.index <= end_date)] + 10\n",
    "df_tCur_sliced = df_tCur[(df_tCur.index >= start_date) & (df_tCur.index <= end_date)]\n",
    "df_tSet_sliced = df_tSet[(df_tSet.index >= start_date) & (df_tSet.index <= end_date)]\n",
    "\n",
    "legendList = ['offON', 'offON_virtual', 'coolHeat', 'tCur', 'tSet']\n",
    "Util.plot_dfList_devID_tagCD([df_offON_sliced, df_offON_virtual_sliced, df_coolHeat_sliced, df_tCur_sliced, df_tSet_sliced], legendList, device_name, createFig=True, W=None, H=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import B0001_Aircon_DataProcessing_241103 as DP\n",
    "\n",
    "df_raw = pd.DataFrame(data=tag_data[0]['colec_val'].values, index=tag_data[0]['colec_dt'], columns=['value'])\n",
    "#df_raw = df_raw.iloc[int(len(df_raw)/2-1500):-1].copy()\n",
    "print(f\"timestamp: {df_raw.index[0]}, {df_raw.index[-1]}\")\n",
    "#df_interpol = Util.resample_time_index_interpolate_NaN_df(df_raw, '15min', 'linear')\n",
    "#Util.plot_interpolated_data(df_raw, df_interpol, device_name, createFig=True)\n",
    "#Util.plot_data(df_raw, plotType='simple', title=None, W=10, H=5)\n",
    "\n",
    "X_df, y_df, nan_counts_df, missing_ratio = DP.preprocess(df_raw, points=4, freqInterval='15min')\n",
    "print(f\"nan_counts max= {nan_counts_df.max()}, {missing_ratio= }\")\n",
    "\n",
    "# 학습 및 테스트 데이터 분할\n",
    "# 시계열 데이터는 시간 순으로 되어 있어야 하고, shuffle=False로 순방향 데이터검증 보장\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3, random_state=42, shuffle=False)\n",
    "#train_size = int(len(X) * 0.8)\n",
    "#X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "#y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# LightGBM 데이터 세트 생성\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 50,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "model_a = lgb.train(params,\n",
    "                    train_data,\n",
    "                    valid_sets=[valid_data],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "##################\n",
    "# 예측 수행\n",
    "y_pred_a = model_a.predict(X_test, num_iteration=model_a.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_a = root_mean_squared_error(y_test, y_pred_a)\n",
    "print(f'Experiment A: {rmse_a= :.2f} with {X_df.shape[1]} features')\n",
    "\n",
    "# 예측 결과 시각화\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_raw.index, y=df_raw['value'], mode='lines', name='Actual Value'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='Cleaned Value'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_pred_a, mode='lines', name='Predicted Value'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Actual vs Predicted Values',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Values',\n",
    "                  legend_title='Legend',\n",
    "                  legend=dict(x=0.5, y=0.9, xanchor='center', yanchor='bottom', orientation='h')\n",
    "                  )\n",
    "fig.show()\n",
    "\n",
    "# Feature Importance Visualization\n",
    "# 모델의 특성 중요도 추출\n",
    "feature_importances = model_a.feature_importance()\n",
    "feature_names = X_df.columns\n",
    "\n",
    "# 특성 중요도를 데이터프레임으로 정리\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 특성 중요도 시각화\n",
    "plt.figure(figsize=(10, 12))  # 그래프 크기 설정\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])  # 수평 막대그래프 생성\n",
    "plt.xlabel('Feature Importance')  # x축 레이블 설정\n",
    "plt.title('Feature Importance Visualization')  # 그래프 제목 설정\n",
    "plt.gca().invert_yaxis()  # 중요도가 높은 순으로 표시\n",
    "plt.show()  # 그래프 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 보조 Step. Features Reduction (Experiment B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment B. Feature Reduction을 적용 후 (Embedded Method with lightgbm 적용)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 기존 모델과 동일한 하이퍼파라미터 설정\n",
    "lgb_estimator = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=50,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# SelectFromModel을 사용하여 중요하지 않은 특성 제거\n",
    "selector = SelectFromModel(estimator=lgb_estimator, threshold='median')\n",
    "\n",
    "# 훈련 데이터에 대해 fit\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# 선택된 특성 이름 추출\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "# 선택된 특성으로 데이터셋 변환\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# LightGBM 데이터 세트 생성 (선택된 특성 사용)\n",
    "train_data_selected = lgb.Dataset(X_train_selected, label=y_train)\n",
    "valid_data_selected = lgb.Dataset(X_test_selected, label=y_test, reference=train_data_selected)\n",
    "\n",
    "# 모델 재학습 (선택된 특성 사용)\n",
    "model_b = lgb.train(params,\n",
    "                    train_data_selected,\n",
    "                    valid_sets=[valid_data_selected],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_b = model_b.predict(X_test_selected, num_iteration=model_b.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_b = root_mean_squared_error(y_test, y_pred_b)\n",
    "print(f'Experiment A: {rmse_a= :.2f} with {X_df.shape[1]} features')\n",
    "print(f'Experiment B: {rmse_b= :.2f} with {X_train_selected.shape[1]} features')\n",
    "\n",
    "# 선택된 특성 중요도 시각화\n",
    "feature_importances_b = model_b.feature_importance()\n",
    "importance_df_b = pd.DataFrame({'feature': selected_features, 'importance': feature_importances_b})\n",
    "importance_df_b = importance_df_b.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(importance_df_b['feature'], importance_df_b['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance after Embedded Method')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "importance_df_b_sort = importance_df_b.sort_index()\n",
    "features_b = importance_df_b_sort['feature'].values\n",
    "#print(f'{importance_df_b= }')\n",
    "print(f'{features_b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 보조 Step. Hyperparameter Optimization (Optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna를 이용한 \n",
    "\n",
    "import os, optuna\n",
    "\n",
    "# optuna_study.db 삭제 여부\n",
    "if True:\n",
    "    file_path_optuna_study_db = r\"/home/ymatics/CodingSpace/2024_AI_BEMS/optuna_study.db\"\n",
    "    if os.path.exists(file_path_optuna_study_db):\n",
    "        os.remove(file_path_optuna_study_db)\n",
    "\n",
    "# 학습 및 테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42, shuffle=False)\n",
    "#X_train, X_test, y_train, y_test = X_train_selected, X_test_selected, y_train, y_test\n",
    "\n",
    "# LightGBM 데이터 세트 생성\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정 및 optuna를 이용한 튜닝\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = lgb.train(params,\n",
    "                      train_data,\n",
    "                      valid_sets=[valid_data],\n",
    "                      num_boost_round=1000,\n",
    "                      valid_names=['validation'],\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
    "    \n",
    "    # 예측 수행\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    return rmse\n",
    "\n",
    "# 최적화 수행\n",
    "# Optuna 스터디 생성 (SQLite 스토리지 사용)\n",
    "STUDY_NAME = \"optuna_study\"\n",
    "DB_PATH = \"sqlite:///optuna_study.db\"  # SQLite 데이터베이스 파일 경로\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=DB_PATH,\n",
    "    direction='minimize',\n",
    "    load_if_exists=True  # 기존 스터디가 있으면 불러옴\n",
    ")\n",
    "study.optimize(objective, n_trials=10)  # 30\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "# 최적 하이퍼파라미터로 모델 재학습\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'rmse'\n",
    "best_params['boosting_type'] = 'gbdt'\n",
    "\n",
    "model_opt = lgb.train(best_params,\n",
    "                    train_data,\n",
    "                    valid_sets=[valid_data],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_opt = model_opt.predict(X_test, num_iteration=model_opt.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_opt = root_mean_squared_error(y_test, y_pred_opt)\n",
    "\n",
    "print('*'*30) \n",
    "print(f'Experiment(Optimization): {rmse_opt= :.2f} with {X_train.shape[1]} features')\n",
    "print(f'{model_opt.params= }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model & Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingID, devID, tagCD = buildingID, devID, list(tag_dict.keys())[0]\n",
    "file_path_AI_model = f\"/home/ymatics/CodingSpace/2024_AI_BEMS/B0001_AI_model/{buildingID}_{devID}_{tagCD}_FAN_Anomaly_LGBM.txt\"\n",
    "model_a.save_model(file_path_AI_model)\n",
    "#model_opt.save_model(file_path_AI_model)\n",
    "\n",
    "# 모델 로드\n",
    "model_infer = lgb.Booster(model_file=file_path_AI_model)\n",
    "\n",
    "# 예측 수행 (로드된 모델 사용)\n",
    "y_pred_a = model_infer.predict(X_test)\n",
    "\n",
    "# 모델 평가\n",
    "rmse_loaded = root_mean_squared_error(y_test, y_pred_a)\n",
    "print(f'RMSE (Loaded Model): {rmse_loaded= :.2f}')\n",
    "print(f'{file_path_AI_model= }')\n",
    "\n",
    "# RMSE 평가\n",
    "p_ = 4\n",
    "rmse_over_time = np.sqrt((y_test - y_pred_a)**2)\n",
    "daily_rmse = rmse_over_time.resample('1d').mean()\n",
    "daily_rmse = rmse_over_time.rolling(window=p_ * 24).mean()\n",
    "\n",
    "# 예측 결과 시각화\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for values\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='Cleaned Value'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_pred_a, mode='lines', name='Predicted Value'))\n",
    "\n",
    "# Add RMSE trace on secondary y-axis\n",
    "fig.add_trace(go.Scatter(x=daily_rmse.index, y=daily_rmse, mode='lines',\n",
    "                        name='Daily RMSE', yaxis='y2'))\n",
    "\n",
    "# Update layout with secondary y-axis\n",
    "fig.update_layout(\n",
    "    title='Time Series Anomaly Detection with Daily RMSE (Loaded Model)', \n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Values',\n",
    "    yaxis=dict(\n",
    "        range=[-10, 400]\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Daily RMSE',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        range=[-10, 400]\n",
    "    ),\n",
    "    legend_title='Legend',\n",
    "    legend=dict(x=0.5, y=0.9, xanchor='center', yanchor='bottom', orientation='h')\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Power Peak Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import B0001_MainPower_DataProcessing_241103 as DP\n",
    "import Utility as Util\n",
    "\n",
    "df_interpol, df_is_missing, nan_counts_df, missing_ratio = DP.preprocess(df_raw, points=4, freqInterval='15min', only_cleansing=True, fill_method='zero')\n",
    "#Util.plot_df_raw_interpololated_data(df_raw, df_interpol, plotType='plotly', title=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 기간 데이터에 대해서 일별 최대값과 해당 시간 인덱스를 찾아 통계 처리\n",
    "\n",
    "start_date, end_date = '2023-01-01', '2023-01-30' # 2024-10-14\n",
    "df_period = df_interpol[(df_interpol.index >= start_date) & (df_interpol.index <= end_date)].copy()\n",
    "\n",
    "# 일별 최대값과 해당 시간 인덱스를 찾기\n",
    "daily_peaks = df_period.resample('D').apply(lambda x: x['value'].idxmax())\n",
    "daily_peak_values = df_period.resample('D').apply(lambda x: x['value'].max())\n",
    "\n",
    "# 결과를 DataFrame으로 결합\n",
    "daily_peak_info = pd.DataFrame({\n",
    "    'Peak Time': daily_peaks,\n",
    "    'Peak Value': daily_peak_values\n",
    "})\n",
    "\n",
    "# 피크 시간에서 시간과 분을 추출\n",
    "daily_peak_info['Peak Time'] = daily_peak_info['Peak Time'].dt.strftime('%H:%M')\n",
    "# 00:00 시의 피크 시간 제외\n",
    "daily_peak_info = daily_peak_info[daily_peak_info['Peak Time'] != '00:00']\n",
    "\n",
    "# 피크 시간의 빈도 계산\n",
    "peak_time_freq = daily_peak_info['Peak Time'].value_counts().sort_index()\n",
    "# 각 시간대의 최대 피크 값 계산\n",
    "peak_time_max_values = daily_peak_info.groupby('Peak Time')['Peak Value'].max().astype(int).sort_index()\n",
    "\n",
    "Util.plot_peak_time_distribution(peak_time_freq, peak_time_max_values, start_date, end_date)\n",
    "\"\"\"2023-01-03 12:30:00에 최대치 521\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_peak_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
