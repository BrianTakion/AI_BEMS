{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp(first~last): 2023-01-01 00:00:00 ~ 2024-10-14 10:00:00\n",
      "timestamp(min~max):    2023-01-01 00:00:00 ~ 2024-10-14 10:00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "#from lightgbm import LGBMRegressor  # sckit에서 lightgbm을 wrapping한 것\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import Utility as Util\n",
    "\n",
    "########\n",
    "buildingID, buildingName = 'B0001', '해양과학고'\n",
    "file_path = '/home/ymatics/CodingSpace/2024_AI_BEMS/241014_data_colec_h/data_colec_h_202410141025_B0001.csv'\n",
    "#########\n",
    "df_buildID = pd.read_csv(file_path)\n",
    "df_buildID['colec_dt'] = pd.to_datetime(df_buildID['colec_dt']).dt.floor('min')  # 분 이하는 제거\n",
    "print(f\"timestamp(first~last): {df_buildID['colec_dt'].iloc[0]} ~ {df_buildID['colec_dt'].iloc[-1]}\")\n",
    "print(f\"timestamp(min~max):    {df_buildID['colec_dt'].min()} ~ {df_buildID['colec_dt'].max()}\")\n",
    "\n",
    "start_date, end_date = pd.to_datetime('2023-01-01 00:00:00'), pd.to_datetime('2024-10-14 10:00:00')\n",
    "df_raw_all = df_buildID[(df_buildID['colec_dt'] >= start_date) & (df_buildID['colec_dt'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### devID, tagCD extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devID, device_name = 2001, '학교MAIN'\n",
    "\n",
    "Util.print_tagCD(df_raw_all, devID)\n",
    "tag_dict = {30001: '현재 출력'}\n",
    "tag_dict = {key: f\"{value}@{device_name}\" for key, value in tag_dict.items()}\n",
    "# dictionary를 이용해 각 태그 데이터를 선택\n",
    "tags = {key: Util.select_devID_tagCD(df_raw_all, devID, tagCD=key) for key in tag_dict.keys()}\n",
    "# 특정 태그에 대해 추가 연산 수행 (예: 전체누적사용량 차이 계산)\n",
    "#tags[40004] = calc_tagCD_diff(tags[40004])  # '전체누적사용량_차'\n",
    "\n",
    "# 그래프를 그리기 위해 필요한 데이터를 리스트로 변환\n",
    "tag_data = [tags[key] for key in tag_dict.keys()]\n",
    "tag_names = [tag_dict[key] for key in tag_dict.keys()]\n",
    "Util.plot_dfL_devID_tagCD(tag_data, tag_names, device_name, createFig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step A. Feature Engineering, lightGBM Modeling\n",
    "  - LightGBM은 tree-based 모델로 scaling을 특별히 요구하지 않음\n",
    "  - Prediction model 일 때는, 과거 데이터 정보(X)로만 현재 데이터(y)를 Regression 할 수 있도록 함\n",
    "  - 일반적인 Regression model 일 때는, 타 데이터의 현재 데이터(X)를 사용해도 됨\n",
    "  - 참고로, LSTM은 (t-k, ..., t-1) ---> t 에서 별도의 feature engineering을 수행하지 않음 \n",
    "  - train_test_split() 에서, prediction model에서는 shuffle=False, regression model에서는 shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import B0001_MainPower_DataProcessing_241103 as DP\n",
    "\n",
    "df_raw = pd.DataFrame(data=tag_data[0]['colec_val'].values, index=tag_data[0]['colec_dt'], columns=['value'])\n",
    "#df_raw = df_raw.iloc[int(len(df_raw)/2-1500):-1].copy()\n",
    "print(f\"timestamp: {df_raw.index[0]}, {df_raw.index[-1]}\")\n",
    "#df_interpol = Util.resample_time_index_interpolate_NaN_df(df_raw, '15min', 'linear')\n",
    "#Util.plot_interpolated_data(df_raw, df_interpol, device_name, createFig=True)\n",
    "#Util.plot_data(df_raw, plotType='simple', title=None, W=10, H=5)\n",
    "\n",
    "X_df, y_df, nan_counts_df, missing_ratio = DP.preprocess(df_raw, points=4, freqInterval='15min')\n",
    "print(f\"nan_counts max= {nan_counts_df.max()}, {missing_ratio= }\")\n",
    "\n",
    "# 학습 및 테스트 데이터 분할\n",
    "# 시계열 데이터는 시간 순으로 되어 있어야 하고, shuffle=False로 순방향 데이터검증 보장\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3, random_state=42, shuffle=False)\n",
    "#train_size = int(len(X) * 0.8)\n",
    "#X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "#y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# LightGBM 데이터 세트 생성\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 50,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "model_a = lgb.train(params,\n",
    "                    train_data,\n",
    "                    valid_sets=[valid_data],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "##################\n",
    "# 예측 수행\n",
    "y_pred_a = model_a.predict(X_test, num_iteration=model_a.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_a = root_mean_squared_error(y_test, y_pred_a)\n",
    "print(f'Experiment A: {rmse_a= :.2f} with {X_df.shape[1]} features')\n",
    "\n",
    "# 예측 결과 시각화\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_raw.index, y=df_raw['value'], mode='lines', name='Actual Value'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='Cleaned Value'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_pred_a, mode='lines', name='Predicted Value'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Actual vs Predicted Values',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Values',\n",
    "                  legend_title='Legend',\n",
    "                  legend=dict(x=0.5, y=0.9, xanchor='center', yanchor='bottom', orientation='h')\n",
    "                  )\n",
    "fig.show()\n",
    "\n",
    "# Feature Importance Visualization\n",
    "# 모델의 특성 중요도 추출\n",
    "feature_importances = model_a.feature_importance()\n",
    "feature_names = X_df.columns\n",
    "\n",
    "# 특성 중요도를 데이터프레임으로 정리\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 특성 중요도 시각화\n",
    "plt.figure(figsize=(10, 12))  # 그래프 크기 설정\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])  # 수평 막대그래프 생성\n",
    "plt.xlabel('Feature Importance')  # x축 레이블 설정\n",
    "plt.title('Feature Importance Visualization')  # 그래프 제목 설정\n",
    "plt.gca().invert_yaxis()  # 중요도가 높은 순으로 표시\n",
    "plt.show()  # 그래프 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 보조 Step. Features Reduction (Experiment B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment B. Feature Reduction을 적용 후 (Embedded Method with lightgbm 적용)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 기존 모델과 동일한 하이퍼파라미터 설정\n",
    "lgb_estimator = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=50,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# SelectFromModel을 사용하여 중요하지 않은 특성 제거\n",
    "selector = SelectFromModel(estimator=lgb_estimator, threshold='median')\n",
    "\n",
    "# 훈련 데이터에 대해 fit\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# 선택된 특성 이름 추출\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "# 선택된 특성으로 데이터셋 변환\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# LightGBM 데이터 세트 생성 (선택된 특성 사용)\n",
    "train_data_selected = lgb.Dataset(X_train_selected, label=y_train)\n",
    "valid_data_selected = lgb.Dataset(X_test_selected, label=y_test, reference=train_data_selected)\n",
    "\n",
    "# 모델 재학습 (선택된 특성 사용)\n",
    "model_b = lgb.train(params,\n",
    "                    train_data_selected,\n",
    "                    valid_sets=[valid_data_selected],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_b = model_b.predict(X_test_selected, num_iteration=model_b.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_b = root_mean_squared_error(y_test, y_pred_b)\n",
    "print(f'Experiment A: {rmse_a= :.2f} with {X_df.shape[1]} features')\n",
    "print(f'Experiment B: {rmse_b= :.2f} with {X_train_selected.shape[1]} features')\n",
    "\n",
    "# 선택된 특성 중요도 시각화\n",
    "feature_importances_b = model_b.feature_importance()\n",
    "importance_df_b = pd.DataFrame({'feature': selected_features, 'importance': feature_importances_b})\n",
    "importance_df_b = importance_df_b.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(importance_df_b['feature'], importance_df_b['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance after Embedded Method')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "importance_df_b_sort = importance_df_b.sort_index()\n",
    "features_b = importance_df_b_sort['feature'].values\n",
    "#print(f'{importance_df_b= }')\n",
    "print(f'{features_b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 보조 Step. Hyperparameter Optimization (Optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna를 이용한 \n",
    "\n",
    "import os, optuna\n",
    "\n",
    "# optuna_study.db 삭제 여부\n",
    "if True:\n",
    "    file_path_optuna_study_db = r\"/home/ymatics/CodingSpace/2024_AI_BEMS/optuna_study.db\"\n",
    "    if os.path.exists(file_path_optuna_study_db):\n",
    "        os.remove(file_path_optuna_study_db)\n",
    "\n",
    "# 학습 및 테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42, shuffle=False)\n",
    "#X_train, X_test, y_train, y_test = X_train_selected, X_test_selected, y_train, y_test\n",
    "\n",
    "# LightGBM 데이터 세트 생성\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정 및 optuna를 이용한 튜닝\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = lgb.train(params,\n",
    "                      train_data,\n",
    "                      valid_sets=[valid_data],\n",
    "                      num_boost_round=1000,\n",
    "                      valid_names=['validation'],\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
    "    \n",
    "    # 예측 수행\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    return rmse\n",
    "\n",
    "# 최적화 수행\n",
    "# Optuna 스터디 생성 (SQLite 스토리지 사용)\n",
    "STUDY_NAME = \"optuna_study\"\n",
    "DB_PATH = \"sqlite:///optuna_study.db\"  # SQLite 데이터베이스 파일 경로\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=DB_PATH,\n",
    "    direction='minimize',\n",
    "    load_if_exists=True  # 기존 스터디가 있으면 불러옴\n",
    ")\n",
    "study.optimize(objective, n_trials=10)  # 30\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "# 최적 하이퍼파라미터로 모델 재학습\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'rmse'\n",
    "best_params['boosting_type'] = 'gbdt'\n",
    "\n",
    "model_opt = lgb.train(best_params,\n",
    "                    train_data,\n",
    "                    valid_sets=[valid_data],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_opt = model_opt.predict(X_test, num_iteration=model_opt.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_opt = root_mean_squared_error(y_test, y_pred_opt)\n",
    "\n",
    "print('*'*30) \n",
    "print(f'Experiment(Optimization): {rmse_opt= :.2f} with {X_train.shape[1]} features')\n",
    "print(f'{model_opt.params= }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model & Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingID, devID, tagCD = buildingID, devID, list(tag_dict.keys())[0]\n",
    "file_path_AI_model = f\"/home/ymatics/CodingSpace/2024_AI_BEMS/B0001_AI_model/{buildingID}_{devID}_{tagCD}_FAN_Anomaly_LGBM.txt\"\n",
    "model_a.save_model(file_path_AI_model)\n",
    "#model_opt.save_model(file_path_AI_model)\n",
    "\n",
    "# 모델 로드\n",
    "model_infer = lgb.Booster(model_file=file_path_AI_model)\n",
    "\n",
    "# 예측 수행 (로드된 모델 사용)\n",
    "y_pred_a = model_infer.predict(X_test)\n",
    "\n",
    "# 모델 평가\n",
    "rmse_loaded = root_mean_squared_error(y_test, y_pred_a)\n",
    "print(f'RMSE (Loaded Model): {rmse_loaded= :.2f}')\n",
    "print(f'{file_path_AI_model= }')\n",
    "\n",
    "# RMSE 평가\n",
    "p_ = 4\n",
    "rmse_over_time = np.sqrt((y_test - y_pred_a)**2)\n",
    "daily_rmse = rmse_over_time.resample('1d').mean()\n",
    "daily_rmse = rmse_over_time.rolling(window=p_ * 24).mean()\n",
    "\n",
    "# 예측 결과 시각화\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for values\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='Cleaned Value'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_pred_a, mode='lines', name='Predicted Value'))\n",
    "\n",
    "# Add RMSE trace on secondary y-axis\n",
    "fig.add_trace(go.Scatter(x=daily_rmse.index, y=daily_rmse, mode='lines',\n",
    "                        name='Daily RMSE', yaxis='y2'))\n",
    "\n",
    "# Update layout with secondary y-axis\n",
    "fig.update_layout(\n",
    "    title='Time Series Anomaly Detection with Daily RMSE (Loaded Model)', \n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Values',\n",
    "    yaxis=dict(\n",
    "        range=[-10, 400]\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Daily RMSE',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        range=[-10, 400]\n",
    "    ),\n",
    "    legend_title='Legend',\n",
    "    legend=dict(x=0.5, y=0.9, xanchor='center', yanchor='bottom', orientation='h')\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Power Peak Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import B0001_MainPower_DataProcessing_241103 as DP\n",
    "import Utility as Util\n",
    "\n",
    "df_interpol, df_is_missing, nan_counts_df, missing_ratio = DP.preprocess(df_raw, points=4, freqInterval='15min', only_cleansing=True, fill_method='zero')\n",
    "#Util.plot_df_raw_interpololated_data(df_raw, df_interpol, plotType='plotly', title=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 기간 데이터에 대해서 일별 최대값과 해당 시간 인덱스를 찾아 통계 처리\n",
    "\n",
    "start_date, end_date = '2023-01-01', '2023-01-30' # 2024-10-14\n",
    "df_period = df_interpol[(df_interpol.index >= start_date) & (df_interpol.index <= end_date)].copy()\n",
    "\n",
    "# 일별 최대값과 해당 시간 인덱스를 찾기\n",
    "daily_peaks = df_period.resample('D').apply(lambda x: x['value'].idxmax())\n",
    "daily_peak_values = df_period.resample('D').apply(lambda x: x['value'].max())\n",
    "\n",
    "# 결과를 DataFrame으로 결합\n",
    "daily_peak_info = pd.DataFrame({\n",
    "    'Peak Time': daily_peaks,\n",
    "    'Peak Value': daily_peak_values\n",
    "})\n",
    "\n",
    "# 피크 시간에서 시간과 분을 추출\n",
    "daily_peak_info['Peak Time'] = daily_peak_info['Peak Time'].dt.strftime('%H:%M')\n",
    "# 00:00 시의 피크 시간 제외\n",
    "daily_peak_info = daily_peak_info[daily_peak_info['Peak Time'] != '00:00']\n",
    "\n",
    "# 피크 시간의 빈도 계산\n",
    "peak_time_freq = daily_peak_info['Peak Time'].value_counts().sort_index()\n",
    "# 각 시간대의 최대 피크 값 계산\n",
    "peak_time_max_values = daily_peak_info.groupby('Peak Time')['Peak Value'].max().astype(int).sort_index()\n",
    "\n",
    "Util.plot_peak_time_distribution(peak_time_freq, peak_time_max_values, start_date, end_date)\n",
    "\"\"\"2023-01-03 12:30:00에 최대치 521\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Peak Time</th>\n",
       "      <th>Peak Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colec_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01</th>\n",
       "      <td>08:45</td>\n",
       "      <td>187.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>12:45</td>\n",
       "      <td>259.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>12:30</td>\n",
       "      <td>521.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>09:15</td>\n",
       "      <td>487.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>09:15</td>\n",
       "      <td>493.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>09:15</td>\n",
       "      <td>467.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-07</th>\n",
       "      <td>10:30</td>\n",
       "      <td>184.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-08</th>\n",
       "      <td>15:00</td>\n",
       "      <td>213.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-09</th>\n",
       "      <td>09:00</td>\n",
       "      <td>400.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-10</th>\n",
       "      <td>10:00</td>\n",
       "      <td>383.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-11</th>\n",
       "      <td>10:45</td>\n",
       "      <td>252.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-12</th>\n",
       "      <td>08:45</td>\n",
       "      <td>290.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-13</th>\n",
       "      <td>14:15</td>\n",
       "      <td>207.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-14</th>\n",
       "      <td>14:00</td>\n",
       "      <td>173.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15</th>\n",
       "      <td>13:00</td>\n",
       "      <td>137.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-16</th>\n",
       "      <td>14:45</td>\n",
       "      <td>219.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-17</th>\n",
       "      <td>08:45</td>\n",
       "      <td>241.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-18</th>\n",
       "      <td>15:30</td>\n",
       "      <td>228.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>09:00</td>\n",
       "      <td>230.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-20</th>\n",
       "      <td>14:30</td>\n",
       "      <td>313.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-21</th>\n",
       "      <td>02:00</td>\n",
       "      <td>147.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-22</th>\n",
       "      <td>09:30</td>\n",
       "      <td>177.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-23</th>\n",
       "      <td>06:15</td>\n",
       "      <td>184.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-24</th>\n",
       "      <td>13:45</td>\n",
       "      <td>255.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-25</th>\n",
       "      <td>10:30</td>\n",
       "      <td>391.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-26</th>\n",
       "      <td>15:00</td>\n",
       "      <td>304.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-27</th>\n",
       "      <td>11:15</td>\n",
       "      <td>346.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-28</th>\n",
       "      <td>09:30</td>\n",
       "      <td>221.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-29</th>\n",
       "      <td>16:30</td>\n",
       "      <td>201.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Peak Time  Peak Value\n",
       "colec_dt                        \n",
       "2023-01-01     08:45      187.84\n",
       "2023-01-02     12:45      259.26\n",
       "2023-01-03     12:30      521.28\n",
       "2023-01-04     09:15      487.50\n",
       "2023-01-05     09:15      493.26\n",
       "2023-01-06     09:15      467.99\n",
       "2023-01-07     10:30      184.64\n",
       "2023-01-08     15:00      213.74\n",
       "2023-01-09     09:00      400.07\n",
       "2023-01-10     10:00      383.80\n",
       "2023-01-11     10:45      252.86\n",
       "2023-01-12     08:45      290.34\n",
       "2023-01-13     14:15      207.78\n",
       "2023-01-14     14:00      173.66\n",
       "2023-01-15     13:00      137.61\n",
       "2023-01-16     14:45      219.54\n",
       "2023-01-17     08:45      241.87\n",
       "2023-01-18     15:30      228.36\n",
       "2023-01-19     09:00      230.83\n",
       "2023-01-20     14:30      313.33\n",
       "2023-01-21     02:00      147.10\n",
       "2023-01-22     09:30      177.13\n",
       "2023-01-23     06:15      184.94\n",
       "2023-01-24     13:45      255.48\n",
       "2023-01-25     10:30      391.53\n",
       "2023-01-26     15:00      304.60\n",
       "2023-01-27     11:15      346.75\n",
       "2023-01-28     09:30      221.44\n",
       "2023-01-29     16:30      201.87"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_peak_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
