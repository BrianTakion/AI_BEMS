{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열데이터 예측/분류 모델(lightGBM)\n",
    "* Step A. Features Engineering\n",
    "* Step B. lightGBM modeling\n",
    "* Step C. Features Reduction\n",
    "* Step D. Hyperparameter Optimization\n",
    "* Step E. Save the model\n",
    "\n",
    "* 참고\n",
    "* prophet: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step A. Feature Engineering\n",
    "  - LightGBM은 tree-based 모델로 scaling을 특별히 요구하지 않음\n",
    "  - Prediction model 일 때는, 과거 데이터 정보(X)로만 현재 데이터(y)를 Regression 할 수 있도록 함\n",
    "  - 일반적인 Regression model 일 때는, 타 데이터의 현재 데이터(X)를 사용해도 됨\n",
    "  - 참고로, LSTM은 (t-k, ..., t-1) ---> t 에서 별도의 feature engineering을 수행하지 않음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holidays\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "import lightgbm as lgb\n",
    "#from lightgbm import LGBMRegressor  # sckit에서 lightgbm을 wrapping한 것\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "import datetime\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import Utility as Util\n",
    "\n",
    "file_path = r\"/home/ymatics/CodingSpace/2024_AI_BEMS/df_raw.pickle\"\n",
    "df_raw = pd.read_pickle(file_path)\n",
    "print(f\"timestamp: {df_raw.index[0]}, {df_raw.index[-1]}\")\n",
    "#start_date, end_date = pd.to_datetime('2024-04-01 00:00:00'), pd.to_datetime('2024-07-01 13:00:00')\n",
    "#df = df_raw.loc[start_date:end_date].copy()\n",
    "df = df_raw.copy()\n",
    "Util.plot_data(df, plotType='simple', title=None, W=10, H=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ = 4  # 시간당 4 points 샘플링, 15min 간격\n",
    "freq = '15min'\n",
    "\n",
    "# 1. 누락여부 피쳐 생성 및 보간\n",
    "df.index = df.index.round(freq)  # freq 단위로 라운딩하여 시간 맞추고(15분+=50% time index 흔들림 허용)\n",
    "df = df[~df.index.duplicated(keep='first')]  # 중복된 시간 포인트는 제거\n",
    "df = df.resample(freq).asfreq()  # freq 단위로 인덱스 리샘플링, asfreq()로 빈 시간대는 NaN으로 채움\n",
    "df['is_missing'] = df['target'].isna().astype(int)\n",
    "df.ffill(inplace=True)  # 예측 모델인 경우, 미래 데이터를 사용하지 않도록 보간\n",
    "df.bfill(inplace=True)  # 예측 모델인 경우, 만약 첫 번째 값이 결측치인 경우를 대비\n",
    "#df['target'] = df['target'].interpolate(method='time')  # 회귀 모델인 경우, 대안으로 선택 가능\n",
    "\n",
    "df_shift_1p = df['target'].shift(1)  # Shifted targets to use past data only\n",
    "\n",
    "# 2. 시간 기반 피쳐 생성 (대한민국의 주말 및 공휴일 특징 반영)\n",
    "kr_holidays = holidays.KR()\n",
    "kr_business_day = CustomBusinessDay(holidays=kr_holidays)\n",
    "df['month'] = df.index.month\n",
    "df['day'] = df.index.day\n",
    "df['weekday'] = df.index.weekday\n",
    "df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)  # 주말 여부\n",
    "df['is_holiday'] = df.index.isin(kr_holidays).astype(int)  # 대한민국 공휴일 여부\n",
    "\n",
    "# 2. 명절 연휴 처리(설날, 추석은 전일과 후일이 연휴임)\n",
    "major_holiday_dates = pd.to_datetime([date for date in kr_holidays if kr_holidays[date] in ['설날', '추석']])\n",
    "df.loc[df.index.isin(major_holiday_dates - pd.DateOffset(days=1)), 'is_holiday'] = 1\n",
    "df.loc[df.index.isin(major_holiday_dates + pd.DateOffset(days=1)), 'is_holiday'] = 1\n",
    "\n",
    "# 2. 대체공휴일 보정\n",
    "holiday_dates = pd.to_datetime(list(kr_holidays.keys()))  # kr_holidays를 datetime으로 변환\n",
    "for date in kr_holidays:\n",
    "    if date.weekday() in [5, 6]:\n",
    "        replacement_date = date + datetime.timedelta(days=1)\n",
    "        while replacement_date.weekday() in [5, 6] or replacement_date in kr_holidays:\n",
    "            replacement_date += datetime.timedelta(days=1)\n",
    "        df.loc[replacement_date, 'is_holiday'] = 1\n",
    "\n",
    "# 3. 시간 지연 피쳐 생성\n",
    "df['lag_1p'] = df['target'].shift(1)  # 1h delayed pattern\n",
    "df['lag_2p'] = df['target'].shift(2)  \n",
    "df['lag_3p'] = df['target'].shift(3)  \n",
    "df['lag_4p'] = df['target'].shift(4)  \n",
    "df['lag_1d_0p'] = df['target'].shift(p_ * 24 + 0)  # 1day = 4p_ x 24h/p_ delayed pattern\n",
    "df['lag_1d_1p'] = df['target'].shift(p_ * 24 + 1)  \n",
    "df['lag_1d_2p'] = df['target'].shift(p_ * 24 + 2)  \n",
    "df['lag_1d_3p'] = df['target'].shift(p_ * 24 + 3)  \n",
    "df['lag_1d_4p'] = df['target'].shift(p_ * 24 + 4)  \n",
    "df['lag_1w_0p'] = df['target'].shift(p_ * 24 * 7 + 0)  # 1w = 4p_ x 24h/p_ x 7days delayed pattern\n",
    "df['lag_1w_1p'] = df['target'].shift(p_ * 24 * 7 + 1)  \n",
    "df['lag_1w_2p'] = df['target'].shift(p_ * 24 * 7 + 2)  \n",
    "df['lag_1w_3p'] = df['target'].shift(p_ * 24 * 7 + 3)  \n",
    "df['lag_1w_4p'] = df['target'].shift(p_ * 24 * 7 + 4) \n",
    "\n",
    "# 4. 변동률 및 변동률의 변동률 피쳐 생성\n",
    "epsilon = 1e-3\n",
    "shifted_1p = df['target'].shift(1)  # Shifted targets to use past data only\n",
    "divisor_1p = np.where(np.abs(shifted_1p) > epsilon, shifted_1p, np.sign(shifted_1p) * epsilon)\n",
    "df['rate'] = (shifted_1p - shifted_1p.shift(1)) / divisor_1p\n",
    "df['diff_rate'] = df['rate'].diff()\n",
    "\n",
    "shifted_1d = df['target'].shift(p_ * 24)  # 하루 전의 동일 시간대 변동률의 변동률\n",
    "divisor_1d = np.where(np.abs(shifted_1d) > epsilon, shifted_1d, np.sign(shifted_1d) * epsilon)\n",
    "df['rate_1d'] = (shifted_1d - shifted_1d.shift(1)) / divisor_1d\n",
    "df['diff_rate_1d'] = df['rate_1d'].diff()\n",
    "\n",
    "# 5. 윈도우 통계 피처 생성\n",
    "df['ma_1h'] = df_shift_1p.rolling(window=p_).mean()  # 1시간 통계, shift(1) 적용으로 과거 데이터 사용\n",
    "df['max_1h'] = df_shift_1p.rolling(window=p_).max()  \n",
    "df['min_1h'] = df_shift_1p.rolling(window=p_).min()  \n",
    "df['std_1h'] = df_shift_1p.rolling(window=p_).std() \n",
    "\n",
    "df['ma_1d'] = df_shift_1p.rolling(window=p_ * 24).mean()  # 1일 통계\n",
    "df['max_1d'] = df_shift_1p.rolling(window=p_ * 24).max()  \n",
    "df['min_1d'] = df_shift_1p.rolling(window=p_ * 24).min()  \n",
    "df['std_1d'] = df_shift_1p.rolling(window=p_ * 24).std() \n",
    "\n",
    "df['p1d_ma_1d'] = df['target'].shift(p_*24).rolling(window=p_ * 24).mean()  # 1일전 동시간대 1day 통계\n",
    "df['p1d_max_1d'] = df['target'].shift(p_*24).rolling(window=p_ * 24).max()  \n",
    "df['p1d_min_1d'] = df['target'].shift(p_*24).rolling(window=p_ * 24).min()  \n",
    "df['p1d_std_1d'] = df['target'].shift(p_*24).rolling(window=p_ * 24).std() \n",
    "\n",
    "df['p1w_ma_1d'] = df['target'].shift(p_*24*7).rolling(window=p_ * 24).mean()  # 1주일전 동시간대 1day 통계\n",
    "df['p1w_max_1d'] = df['target'].shift(p_*24*7).rolling(window=p_ * 24).max()  \n",
    "df['p1w_min_1d'] = df['target'].shift(p_*24*7).rolling(window=p_ * 24).min()  \n",
    "df['p1w_std_1d'] = df['target'].shift(p_*24*7).rolling(window=p_ * 24).std() \n",
    "\n",
    "# 6. 이동 평균의 변화율 특징 생성\n",
    "epsilon = 1e-3\n",
    "shifted_1p = df['ma_1h']\n",
    "divisor_1p = np.where(np.abs(shifted_1p) > epsilon, shifted_1p, np.sign(shifted_1p) * epsilon)\n",
    "df['rate_ma_1h'] = (shifted_1p - shifted_1p.shift(1)) / divisor_1p\n",
    "\n",
    "shifted_1p = df['ma_1d']\n",
    "divisor_1p = np.where(np.abs(shifted_1p) > epsilon, shifted_1p, np.sign(shifted_1p) * epsilon)\n",
    "df['rate_ma_1d'] = (shifted_1p - shifted_1p.shift(1)) / divisor_1p\n",
    "\n",
    "# 7. 추세 및 계절성 분포\n",
    "df['trend'] = np.arange(len(df))\n",
    "df['season'] = df_shift_1p - df['ma_1d']  # shift(1) 적용으로 과거 데이터 사용\n",
    "\n",
    "# 8. 계절성 피처 생성\n",
    "df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# 9. 주기성 특성 (Cyclical Features)\n",
    "df['sine_day'] = np.sin(2 * np.pi * df.index.dayofyear / 365.25)\n",
    "df['cosine_day'] = np.cos(2 * np.pi * df.index.dayofyear / 365.25)\n",
    "\n",
    "# 10. 이상치 여부를 나타내는 특징 생성\n",
    "Q1 = df_shift_1p.rolling(window=p_*24).quantile(0.25)\n",
    "Q3 = df_shift_1p.rolling(window=p_*24).quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df['is_outlier'] = ((df_shift_1p < (Q1 - 1.5 * IQR)) | (df_shift_1p > (Q3 + 1.5 * IQR))).astype(int)  # 이상치 여부\n",
    "\n",
    "# 11. Fourier Transform 특징 생성\n",
    "fft_features = fft(df_shift_1p.fillna(0).values)\n",
    "df['fft_real'] = np.real(fft_features)  # 실수 부분\n",
    "df['fft_imag'] = np.imag(fft_features)  # 허수 부분\n",
    "\n",
    "# 12. 추가적인 계절성 특징 생성\n",
    "df['quarter'] = df.index.quarter  # 분기\n",
    "df['hour'] = df.index.hour        # 시간대\n",
    "df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# 12. 외부 데이터 통합 (예시: 기온 데이터)\n",
    "# 예를 들어, 'temperature.csv' 파일에 날짜별 기온 데이터가 있다고 가정\n",
    "# temperature_df = pd.read_csv('temperature.csv', index_col='date', parse_dates=True)\n",
    "# df = df.join(temperature_df, on='date')  # 기온 데이터 통합\n",
    "\n",
    "# 13. 지수 이동 평균 (Exponential Moving Average)\n",
    "df['ema_1d'] = df_shift_1p.ewm(span=p_*24, adjust=False).mean()  # 이전 1일 지수 이동 평균\n",
    "\n",
    "# 14. 엔트로피 특징 생성\n",
    "def calc_entropy(x):\n",
    "    counts = np.histogram(x.dropna(), bins=10)[0]\n",
    "    return entropy(counts)\n",
    "df['entropy_1d'] = df_shift_1p.rolling(window=p_*24).apply(calc_entropy)  # 1일 엔트로피\n",
    "\n",
    "# 15. 누적 합계 및 변화율\n",
    "df['cum_sum'] = df_shift_1p.cumsum()  # 누적 합계\n",
    "df['cum_pct'] = df['cum_sum'].pct_change()  # 누적 합계의 증감율\n",
    "\n",
    "nan_counts = df.isna().sum()  # 각 열마다 NaN의 갯수 출력\n",
    "for index, item in enumerate(nan_counts):\n",
    "    print(f\"{df.columns[index]= }, NaN Count: {item}\")\n",
    "\n",
    "# 결측값 처리 (피처 생성시 시간 지연으로 인해 발생)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 피처벡터와 타겟 분리\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step B. lightGBM Modeling\n",
    "- train_test_split() 에서, prediction model에서는 shuffle=False, regression model에서는 shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터 분할\n",
    "# 시계열 데이터는 시간 순으로 되어 있어야 하고, shuffle=False로 순방향 데이터검증 보장\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "#train_size = int(len(X) * 0.8)\n",
    "#X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "#y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# LightGBM 데이터 세트 생성\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 30,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "model_a = lgb.train(params,\n",
    "                    train_data,\n",
    "                    valid_sets=[valid_data],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "##################\n",
    "# 예측 수행\n",
    "y_pred_a = model_a.predict(X_test, num_iteration=model_a.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_a = root_mean_squared_error(y_test, y_pred_a)\n",
    "print(f'Experiment A: {rmse_a= :.2f} with {X.shape[1]} features')\n",
    "\n",
    "# 예측 결과 시각화\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['target'], mode='lines', name='Actual Target'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='Actual Test'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_pred_a, mode='lines', name='Predicted'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Actual vs Predicted Values',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Values',\n",
    "                  legend_title='Legend')\n",
    "fig.show()\n",
    "\n",
    "# Feature Importance Visualization\n",
    "# 모델의 특성 중요도 추출\n",
    "feature_importances = model_a.feature_importance()\n",
    "feature_names = X.columns\n",
    "\n",
    "# 특성 중요도를 데이터프레임으로 정리\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 특성 중요도 시각화\n",
    "plt.figure(figsize=(10, 12))  # 그래프 크기 설정\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])  # 수평 막대그래프 생성\n",
    "plt.xlabel('Feature Importance')  # x축 레이블 설정\n",
    "plt.title('Feature Importance Visualization')  # 그래프 제목 설정\n",
    "plt.gca().invert_yaxis()  # 중요도가 높은 순으로 표시\n",
    "plt.show()  # 그래프 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C. Features Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment B. Feature Reduction을 적용 후 (Embedded Method with lightgbm 적용)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 기존 모델과 동일한 하이퍼파라미터 설정\n",
    "lgb_estimator = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=30,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# SelectFromModel을 사용하여 중요하지 않은 특성 제거\n",
    "selector = SelectFromModel(estimator=lgb_estimator, threshold='median')\n",
    "\n",
    "# 훈련 데이터에 대해 fit\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# 선택된 특성 이름 추출\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "# 선택된 특성으로 데이터셋 변환\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# LightGBM 데이터 세트 생성 (선택된 특성 사용)\n",
    "train_data_selected = lgb.Dataset(X_train_selected, label=y_train)\n",
    "valid_data_selected = lgb.Dataset(X_test_selected, label=y_test, reference=train_data_selected)\n",
    "\n",
    "# 모델 재학습 (선택된 특성 사용)\n",
    "model_b = lgb.train(params,\n",
    "                    train_data_selected,\n",
    "                    valid_sets=[valid_data_selected],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_b = model_b.predict(X_test_selected, num_iteration=model_b.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_b = root_mean_squared_error(y_test, y_pred_b)\n",
    "print(f'Experiment A: {rmse_a= :.2f} with {X.shape[1]} features')\n",
    "print(f'Experiment B: {rmse_b= :.2f} with {X_train_selected.shape[1]} features')\n",
    "\n",
    "# 선택된 특성 중요도 시각화\n",
    "feature_importances_b = model_b.feature_importance()\n",
    "importance_df_b = pd.DataFrame({'feature': selected_features, 'importance': feature_importances_b})\n",
    "importance_df_b = importance_df_b.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(importance_df_b['feature'], importance_df_b['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance after Embedded Method')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step D. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# 학습 및 테스트 데이터 분할\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = X_train_selected, X_test_selected, y_train, y_test\n",
    "\n",
    "# LightGBM 데이터 세트 생성\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정 및 optuna를 이용한 튜닝\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = lgb.train(params,\n",
    "                      train_data,\n",
    "                      valid_sets=[valid_data],\n",
    "                      num_boost_round=1000,\n",
    "                      valid_names=['validation'],\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
    "    \n",
    "    # 예측 수행\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    return rmse\n",
    "\n",
    "# 최적화 수행\n",
    "# Optuna 스터디 생성 (SQLite 스토리지 사용)\n",
    "STUDY_NAME = \"optuna_study\"\n",
    "DB_PATH = \"sqlite:///optuna_study.db\"  # SQLite 데이터베이스 파일 경로\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=DB_PATH,\n",
    "    direction='minimize',\n",
    "    load_if_exists=True  # 기존 스터디가 있으면 불러옴\n",
    ")\n",
    "study.optimize(objective, n_trials=10)\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "# 최적 하이퍼파라미터로 모델 재학습\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'rmse'\n",
    "best_params['boosting_type'] = 'gbdt'\n",
    "\n",
    "model_opt = lgb.train(best_params,\n",
    "                    train_data,\n",
    "                    valid_sets=[valid_data],\n",
    "                    num_boost_round=1000,\n",
    "                    valid_names=['validation'],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_opt = model_opt.predict(X_test, num_iteration=model_opt.best_iteration)\n",
    "\n",
    "# RMSE 출력\n",
    "rmse_opt = root_mean_squared_error(y_test, y_pred_opt)\n",
    "\n",
    "print('*'*30) \n",
    "print(f'Experiment(Optimization): {rmse_opt= :.2f} with {X_train_selected.shape[1]} features')\n",
    "print(f'{model_opt.params= }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step E: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.save_model('lightgbm_model_a.txt')\n",
    "model_b.save_model('lightgbm_model_b.txt')\n",
    "model_opt.save_model('lightgbm_model_opt.txt')\n",
    "\n",
    "# 모델 로드\n",
    "model_opt_infer = lgb.Booster(model_file='lightgbm_model_opt.txt')\n",
    "\n",
    "# 예측 수행 (로드된 모델 사용)\n",
    "y_pred_loaded = model_opt_infer.predict(X_test)\n",
    "\n",
    "# 모델 평가\n",
    "rmse_loaded = root_mean_squared_error(y_test, y_pred_loaded)\n",
    "print(f'RMSE (Loaded Model): {rmse_loaded= :.3f}')\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(y_test.index, y_test, label='Actual')\n",
    "plt.plot(y_test.index, y_pred_loaded, label='Predicted (Loaded Model)', color='orange')\n",
    "plt.legend()\n",
    "plt.title('LightGBM Time Series Prediction (Loaded Model)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM prediction: with missing intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Utility as Util\n",
    "\n",
    "file_path = r\"/home/ymatics/CodingSpace/2024_AI_BEMS/df_raw.pickle\"\n",
    "df_raw = pd.read_pickle(file_path)\n",
    "print(f\"timestamp: {df_raw.index[0]}, {df_raw.index[-1]}\")\n",
    "#start_date, end_date = pd.to_datetime('2024-04-01 00:00:00'), pd.to_datetime('2024-07-01 13:00:00')\n",
    "#df = df_raw.loc[start_date:end_date].copy()\n",
    "df = df_raw.copy()\n",
    "Util.plot_data(df, plotType='simple', title=None, W=10, H=5)\n",
    "\n",
    "#df_interpol = Util.resample_time_index_interpolate_NaN_df(df_raw, '15min', 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 예제 데이터 생성 (랜덤하게 구간 누락)\n",
    "data_o = df.copy()\n",
    "\n",
    "# 2. 일부 구간 누락 생성 함수\n",
    "\n",
    "# 3. 누락 구간 설정\n",
    "\n",
    "# 4. 데이터에서 누락 구간 제거\n",
    "\n",
    "# 5. 데이터 복사\n",
    "data = data_o.copy()\n",
    "\n",
    "# 6. 누락 여부 피처 생성 및 보간\n",
    "data['is_missing'] = data['target'].isna().astype(int)\n",
    "\n",
    "data['target'] = data['target'].interpolate(method='time')\n",
    "#data.ffill(inplace=True)  # 미래 데이터를 사용하지 않도록 보간 방법 변경\n",
    "#data.bfill(inplace=True)  # 만약 첫 번째 값이 결측치인 경우를 대비\n",
    "\n",
    "# 7. 시간 관련 피처 생성\n",
    "data['hour'] = data.index.hour\n",
    "data['dayofweek'] = data.index.dayofweek\n",
    "data['month'] = data.index.month\n",
    "\n",
    "# 8. 시차 피처 생성\n",
    "data['lag_1'] = data['target'].shift(1)\n",
    "data['lag_24'] = data['target'].shift(24)\n",
    "\n",
    "# 9. 이동 평균 및 표준편차 피처 생성\n",
    "data['rolling_mean_3'] = data['target'].rolling(window=3).mean()\n",
    "data['rolling_std_3'] = data['target'].rolling(window=3).std()\n",
    "\n",
    "# 10. 결측치 제거 (피처 생성 후)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 11. 학습 및 테스트 데이터 분할\n",
    "train_data = data.loc['2023-01-01':'2024-06-01']\n",
    "test_data = data.loc['2024-06-02':'2024-10-14']\n",
    "\n",
    "X_train = train_data.drop('target', axis=1)\n",
    "y_train = train_data['target']\n",
    "X_test = test_data.drop('target', axis=1)\n",
    "y_test = test_data['target']\n",
    "\n",
    "# 12. LightGBM 데이터셋 생성\n",
    "train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "test_dataset = lgb.Dataset(X_test, label=y_test, reference=train_dataset)\n",
    "\n",
    "# 13. 모델 학습\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_dataset, valid_sets=[test_dataset], callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "# 14. 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# 15. 데이터 통합 및 시각화 준비\n",
    "# y_test와 y_pred를 전체 시간 구간에 맞춰서 재구성\n",
    "full_dates = data_o.index\n",
    "\n",
    "# Create a DataFrame to hold all series\n",
    "plot_df = pd.DataFrame(index=full_dates)\n",
    "plot_df['Original Data'] = data_o['target']\n",
    "plot_df['Interpolated Data'] = data['target']\n",
    "\n",
    "# y_test and y_pred need to be reindexed to full date range\n",
    "y_test_full = pd.Series(data=y_test.values, index=y_test.index)\n",
    "y_test_full = y_test_full.reindex(full_dates)\n",
    "plot_df['Test Data Actual'] = y_test_full\n",
    "\n",
    "y_pred_full = pd.Series(data=y_pred, index=y_test.index)\n",
    "y_pred_full = y_pred_full.reindex(full_dates)\n",
    "plot_df['Test Data Predicted'] = y_pred_full\n",
    "\n",
    "# 16. Plotly를 이용한 시각화\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['Original Data'], mode='lines', name='Original Data', line=dict(color='orange', width=1)))\n",
    "fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['Interpolated Data'], mode='lines', name='Interpolated Data', line=dict(color='blue', width=1)))\n",
    "fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['Test Data Actual'], mode='lines', name='Test Data Actual', line=dict(color='green', width=2)))\n",
    "fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['Test Data Predicted'], mode='lines', name='Test Data Predicted', line=dict(color='red', width=2, dash='dot')))\n",
    "fig.update_layout(\n",
    "    title='Time Series Data Visualization',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Value',\n",
    "    legend=dict(x=0.01, y=0.99),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 모델 훈련 후 feature importance 시각화\n",
    "lgb.plot_importance(model, max_num_features=20)  # 상위 20개의 중요 특징만 표시\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
